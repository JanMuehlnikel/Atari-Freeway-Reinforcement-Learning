{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Frogger Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Frame Size: Box(0, 255, (128,), uint8)\n",
      "Number Of Actions: 5\n",
      "Possible Actions: \n",
      " [[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "Obervation: \n",
      " (array([143,  20,   5,  40,   6,  45, 147, 127,  28,  33,  39,  63, 100,\n",
      "        85, 120,  86,  87,  52, 135,  60, 133, 119, 252,  36,  35, 213,\n",
      "        19, 118, 188,  11, 164,  69, 229,  71, 217, 216, 123, 200, 184,\n",
      "         6, 124, 119, 171, 154, 255,  91,  91, 195,  80,   0,   0,   4,\n",
      "        40,   0,   0,   0, 234, 254, 234, 254,  29, 254,  51, 254, 157,\n",
      "       254, 157, 254,  84, 254,   1,   0, 115, 115, 149,   0,   0,   0,\n",
      "         0,   0,   4,   4,   1,   1,   0, 253, 155, 253, 255,  11,   0,\n",
      "        75,   0,   1,   8,  63,  30,   0,   0,   0,   0,   0,   0,   0,\n",
      "        46, 253,   0, 115, 255, 198,   1, 255,   0, 163,  58,  80, 255,\n",
      "        80, 255,  28,  12,   0,   0,  91,   0,   0, 124, 250], dtype=uint8), 0.0, False, False, {'lives': 4, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# create env with gymnasium (use ram or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Frogger-ram-v5\", # \"ALE/Frogger-ram-v5\" or \"ALE/Frogger-v5\"\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0, 1, 2]\n",
    "    ) \n",
    "\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Frogger-ram-v5\", render_mode=\"rgb_array\", difficulty = 0, mode = 0) \n",
    "\n",
    "# MODEL Hyperparameters\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTIONS_SIZE = env.action_space.n\n",
    "ACTIONS = list(range(0, ACTIONS_SIZE))\n",
    "LEARNING_RATE = 0.01 # Learning Rate (alpha)\n",
    "\n",
    "# AGENT Hyperparameters (epsilon greedy strategy)\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01 # EPSELON value where exploreation stops\n",
    "EPSILON_DECAY_RATE = 0.995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "GAMMA = 0.99 # Discounting rate (lower -> agent thinks more long term)\n",
    "\n",
    "# TRAINING Hyperparameters\n",
    "RENDER_INTERVALL = 50 # Intervall when the game is rendered\n",
    "TOTAL_EPISODES = 1000000\n",
    "MINI_BATCHES_REPLAY = 64\n",
    "REPLAY_BUFFER_MEMORY = 10000\n",
    "STACKED_FRAMES_SIZE = 4\n",
    "AVERAGE_WINDOW = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Q-Learning Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def build_dqn(input_shape=(STATE_SIZE*STACKED_FRAMES_SIZE,)):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(24, input_shape=input_shape, activation='relu'))\n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(ACTIONS_SIZE, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_dqn(input_shape=(STATE_SIZE*STACKED_FRAMES_SIZE,)):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Reshape((16, 32), input_shape=input_shape))  # Reshape to (16, 32)\n",
    "    model.add(layers.Conv1D(32, 3, activation='relu', padding='same'))\n",
    "    model.add(layers.UpSampling1D(4))  # Upsample to (64, 32)\n",
    "    model.add(layers.Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    model.add(layers.UpSampling1D(2))  # Upsample to (128, 64)\n",
    "    model.add(layers.Conv1D(4, 3, activation='linear', padding='same'))  # Output (128, 4)\n",
    "    model.add(layers.Reshape((128, 4)))  # Reshape to (128, 4)\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stacked Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACKED_FRAMES = deque([np.zeros((STATE_SIZE), dtype=np.int32) for i in range(STACKED_FRAMES_SIZE)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "\n",
    "    if is_new_episode:\n",
    "        # clear stack for new episode\n",
    "        stacked_frames = deque([np.zeros((STATE_SIZE), dtype=np.int32) for i in range(STACKED_FRAMES_SIZE)], maxlen=4)\n",
    "        \n",
    "        # Add the same frame 4 times to the deque since its a new episode\n",
    "        stacked_frames.append(state)\n",
    "        stacked_frames.append(state)\n",
    "        stacked_frames.append(state)\n",
    "        stacked_frames.append(state)\n",
    "        \n",
    "        # Stack the frames with numpy (join all 4 frames)\n",
    "        stacked_frames_array = np.stack(stacked_frames, axis=0)\n",
    "\n",
    "    elif not is_new_episode:\n",
    "        # append new frame and remove oldest frame\n",
    "        stacked_frames.append(state)\n",
    "\n",
    "        # Stack the frames with numpy (join all 4 frames)\n",
    "        stacked_frames_array = np.stack(stacked_frames, axis=0) \n",
    "\n",
    "    return stacked_frames_array, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        # deque that ther are only max REPLAY_BUFFER_MEMORY items in the list\n",
    "        # deque = remove oldest item\n",
    "        self.buffer = deque(maxlen=REPLAY_BUFFER_MEMORY)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        # add item to buffer\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, MINI_BATCHES_REPLAY) # 16 (MINI_BATCHES_REPLAY) samples to retrain the mdoel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DQN Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.EPSILON = EPSILON\n",
    "        self.model = build_dqn()\n",
    "        self.target_model = build_dqn()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        flattend_array_state = np.expand_dims(state.reshape(-1), axis=0)\n",
    "        flattend_array_next_state = np.expand_dims(next_state.reshape(-1), axis=0)\n",
    "        self.memory.add((flattend_array_state, action, reward, flattend_array_next_state, done))\n",
    "    \n",
    "    def predict_action(self, stacked_array):\n",
    "        # flatten array for model\n",
    "        flattend_array = np.expand_dims(stacked_array.reshape(-1), axis=0)\n",
    "\n",
    "        if np.random.rand() <= EPSILON:\n",
    "            return random.randrange(ACTIONS_SIZE)\n",
    "        q_values = self.model.predict(flattend_array)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        print(\"Replay\")\n",
    "        minibatch = self.memory.sample()\n",
    "        i = 0\n",
    "        for state, action, reward, next_state, done in tqdm(minibatch):\n",
    "            i += 1\n",
    "            # Predict Target Q-Values\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            if done:\n",
    "                # If the episode is done the target Q-value for the taken action is set to the received reward\n",
    "                target[0][action] = reward\n",
    "            elif not done:\n",
    "                # If the episode is not done, the target Q-value for the taken action is updated using the Bellman equation\n",
    "                t = self.target_model.predict(next_state, verbose=0)[0]\n",
    "                target[0][action] = reward + GAMMA * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "                \n",
    "        if self.EPSILON > EPSILON_MIN:\n",
    "            self.EPSILON *= EPSILON_DECAY_RATE\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reward Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "reward_line, = plt.plot(rewards_per_episode, label='Reward per Episode')\n",
    "average_line, = plt.plot(average_rewards, label='Moving Average Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Episode Reward and Moving Average Reward over Time')\n",
    "plt.legend()\n",
    "\n",
    "# Display plot in a new window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/1000000, Total Reward: 5.0, Moving AVG. Reward: 5.0, Steps: 122, Epsilon: 1.0\n",
      "Episode: 1/1000000, Total Reward: 4.0, Moving AVG. Reward: 4.5, Steps: 195, Epsilon: 1.0\n",
      "Episode: 2/1000000, Total Reward: 10.0, Moving AVG. Reward: 6.333333333333333, Steps: 145, Epsilon: 1.0\n",
      "Episode: 3/1000000, Total Reward: 6.0, Moving AVG. Reward: 6.25, Steps: 153, Epsilon: 1.0\n",
      "Episode: 4/1000000, Total Reward: 9.0, Moving AVG. Reward: 6.8, Steps: 124, Epsilon: 1.0\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:11<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.0, Steps: 145, Epsilon: 0.99\n",
      "Episode: 6/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.0, Steps: 134, Epsilon: 0.99\n",
      "Episode: 7/1000000, Total Reward: 9.0, Moving AVG. Reward: 7.25, Steps: 159, Epsilon: 0.99\n",
      "Episode: 8/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.222222222222222, Steps: 172, Epsilon: 0.99\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:09<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.3, Steps: 194, Epsilon: 0.99\n",
      "Episode: 10/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.363636363636363, Steps: 138, Epsilon: 0.99\n",
      "Episode: 11/1000000, Total Reward: 9.0, Moving AVG. Reward: 7.5, Steps: 133, Epsilon: 0.99\n",
      "Episode: 12/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.461538461538462, Steps: 145, Epsilon: 0.99\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 13/1000000, Total Reward: 5.0, Moving AVG. Reward: 7.285714285714286, Steps: 149, Epsilon: 0.99\n",
      "Episode: 14/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.333333333333333, Steps: 133, Epsilon: 0.99\n",
      "Episode: 15/1000000, Total Reward: 4.0, Moving AVG. Reward: 7.125, Steps: 240, Epsilon: 0.99\n",
      "Episode: 16/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.117647058823529, Steps: 183, Epsilon: 0.99\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 17/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.111111111111111, Steps: 136, Epsilon: 0.98\n",
      "Episode: 18/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.157894736842105, Steps: 151, Epsilon: 0.98\n",
      "Episode: 19/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.15, Steps: 146, Epsilon: 0.98\n",
      "Episode: 20/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.142857142857143, Steps: 161, Epsilon: 0.98\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 21/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.136363636363637, Steps: 143, Epsilon: 0.98\n",
      "Episode: 22/1000000, Total Reward: 11.0, Moving AVG. Reward: 7.304347826086956, Steps: 147, Epsilon: 0.98\n",
      "Episode: 23/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.291666666666667, Steps: 152, Epsilon: 0.98\n",
      "Episode: 24/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.32, Steps: 165, Epsilon: 0.98\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25/1000000, Total Reward: 10.0, Moving AVG. Reward: 7.423076923076923, Steps: 141, Epsilon: 0.97\n",
      "Episode: 26/1000000, Total Reward: 5.0, Moving AVG. Reward: 7.333333333333333, Steps: 175, Epsilon: 0.97\n",
      "Episode: 27/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.285714285714286, Steps: 196, Epsilon: 0.97\n",
      "Episode: 28/1000000, Total Reward: 10.0, Moving AVG. Reward: 7.379310344827586, Steps: 151, Epsilon: 0.97\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 29/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.366666666666666, Steps: 168, Epsilon: 0.97\n",
      "Episode: 30/1000000, Total Reward: 4.0, Moving AVG. Reward: 7.258064516129032, Steps: 159, Epsilon: 0.97\n",
      "Episode: 31/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.28125, Steps: 146, Epsilon: 0.97\n",
      "Episode: 32/1000000, Total Reward: 10.0, Moving AVG. Reward: 7.363636363636363, Steps: 124, Epsilon: 0.97\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 33/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.382352941176471, Steps: 138, Epsilon: 0.96\n",
      "Episode: 34/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.371428571428571, Steps: 121, Epsilon: 0.96\n",
      "Episode: 35/1000000, Total Reward: 9.0, Moving AVG. Reward: 7.416666666666667, Steps: 130, Epsilon: 0.96\n",
      "Episode: 36/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.378378378378378, Steps: 149, Epsilon: 0.96\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 37/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.342105263157895, Steps: 138, Epsilon: 0.96\n",
      "Episode: 38/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.358974358974359, Steps: 140, Epsilon: 0.96\n",
      "Episode: 39/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.375, Steps: 145, Epsilon: 0.96\n",
      "Episode: 40/1000000, Total Reward: 10.0, Moving AVG. Reward: 7.439024390243903, Steps: 132, Epsilon: 0.96\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 41/1000000, Total Reward: 9.0, Moving AVG. Reward: 7.476190476190476, Steps: 180, Epsilon: 0.95\n",
      "Episode: 42/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.465116279069767, Steps: 139, Epsilon: 0.95\n",
      "Episode: 43/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.431818181818182, Steps: 146, Epsilon: 0.95\n",
      "Episode: 44/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.444444444444445, Steps: 137, Epsilon: 0.95\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 45/1000000, Total Reward: 8.0, Moving AVG. Reward: 7.456521739130435, Steps: 193, Epsilon: 0.95\n",
      "Episode: 46/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.425531914893617, Steps: 153, Epsilon: 0.95\n",
      "Episode: 47/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.395833333333333, Steps: 130, Epsilon: 0.95\n",
      "Episode: 48/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.387755102040816, Steps: 140, Epsilon: 0.95\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 49/1000000, Total Reward: 5.0, Moving AVG. Reward: 7.34, Steps: 145, Epsilon: 0.94\n",
      "Episode: 50/1000000, Total Reward: 6.0, Moving AVG. Reward: 7.36, Steps: 127, Epsilon: 0.94\n",
      "Episode: 51/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.42, Steps: 139, Epsilon: 0.94\n",
      "Episode: 52/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.36, Steps: 176, Epsilon: 0.94\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 53/1000000, Total Reward: 4.0, Moving AVG. Reward: 7.32, Steps: 145, Epsilon: 0.94\n",
      "Episode: 54/1000000, Total Reward: 9.0, Moving AVG. Reward: 7.32, Steps: 115, Epsilon: 0.94\n",
      "Episode: 55/1000000, Total Reward: 5.0, Moving AVG. Reward: 7.26, Steps: 125, Epsilon: 0.94\n",
      "Episode: 56/1000000, Total Reward: 7.0, Moving AVG. Reward: 7.26, Steps: 141, Epsilon: 0.94\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13/64 [00:01<00:06,  7.43it/s]"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    # create env (human to render game and see actions)\n",
    "    if episode % RENDER_INTERVALL == 0 and episode != 0:\n",
    "        env = gym.make(\"ALE/Frogger-ram-v5\", render_mode=\"human\", difficulty = 0, mode = 0) \n",
    "    else:\n",
    "        env = gym.make(\"ALE/Frogger-ram-v5\", render_mode=\"rgb_array\", difficulty = 0, mode = 0) \n",
    "    state = env.reset()[0]\n",
    "\n",
    "    # reset episode variables\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    is_new_episode = True\n",
    "\n",
    "    # initilize episode stack\n",
    "    stacked_array_state, STACKED_FRAMES = stack_frames(STACKED_FRAMES, state, is_new_episode)\n",
    "    \n",
    "    while not done:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        if is_new_episode and step_count <= 110:\n",
    "            # jsut wait for first 100 steps because of initialization \n",
    "            action = 0\n",
    "            next_state, reward, done, loss_of_live, info = env.step(0)\n",
    "        else:\n",
    "            action = agent.predict_action(stacked_array_state)\n",
    "            next_state, reward, done, loss_of_live, info = env.step(action)\n",
    "            is_new_episode = False\n",
    "\n",
    "        # Predict action\n",
    "        action = agent.predict_action(stacked_array_state)\n",
    "        next_state, reward, done, loss_of_live, info = env.step(action)\n",
    "        \n",
    "        # Do Stacking\n",
    "        stacked_array_next_state, STACKED_FRAMES = stack_frames(STACKED_FRAMES, next_state, is_new_episode)\n",
    "\n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # store action infromation in memory\n",
    "        next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "        agent.remember(stacked_array_state, action, reward, stacked_array_next_state, done)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        stacked_array_state = stacked_array_next_state\n",
    "        \n",
    "        # if game finished (won or lose)\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "\n",
    "            rewards_per_episode.append(total_reward)\n",
    "\n",
    "            if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "            else:\n",
    "                moving_average = np.mean(rewards_per_episode)\n",
    "            average_rewards.append(moving_average)\n",
    "\n",
    "            # reward graph\n",
    "            reward_line.set_data(range(episode + 1), rewards_per_episode)\n",
    "            average_line.set_data(range(episode + 1), average_rewards)\n",
    "            plt.xlim(0, episode + 1)\n",
    "            plt.ylim(0, max(max(rewards_per_episode), max(average_rewards)) + 10)\n",
    "            plt.pause(0.01)\n",
    "\n",
    "            print(f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Steps: {step_count}, Epsilon: {agent.EPSILON:.2}\")\n",
    "            break\n",
    "\n",
    "    if episode % 4 == 0 and episode != 0:\n",
    "        agent.replay()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
