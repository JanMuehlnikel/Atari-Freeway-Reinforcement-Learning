{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import models, layers, optimizers\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Frogger Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [104, 104, 104, ..., 104, 104, 104],\n",
      "       [104, 104, 104, ..., 104, 104, 104],\n",
      "       [104, 104, 104, ..., 104, 104, 104]], dtype=uint8), 0.0, False, False, {'lives': 4, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "State Frame Size: Box(0, 255, (210, 160), uint8)\n",
      "Number Of Actions: 5\n",
      "Possible Actions: \n",
      " [[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "Obervation: \n",
      " [[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [104 104 104 ... 104 104 104]\n",
      " [104 104 104 ... 104 104 104]\n",
      " [104 104 104 ... 104 104 104]]\n",
      "(210, 160)\n"
     ]
    }
   ],
   "source": [
    "# create env with gymnasium (use ram or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Frogger-v5\", # \"ALE/Frogger-ram-v5\" or \"ALE/Frogger-v5\"\n",
    "    obs_type=\"grayscale\", # ram, grescale, rgb\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0, 1, 2]\n",
    "    ) \n",
    "\n",
    "env.reset()\n",
    "print(env.step(0))\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation[0]}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Frogger-v5\", obs_type=\"grayscale\", render_mode=\"rgb_array\", difficulty = 0, mode = 0) \n",
    "\n",
    "# MODEL Hyperparameters\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTIONS_SIZE = env.action_space.n\n",
    "ACTIONS = list(range(0, ACTIONS_SIZE))\n",
    "LEARNING_RATE = 0.001 # Learning Rate (alpha)\n",
    "\n",
    "# AGENT Hyperparameters (epsilon greedy strategy)\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.001 # EPSELON value where exploreation stops\n",
    "EPSILON_DECAY_RATE = 0.9995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "GAMMA = 0.95 # Discounting rate (lower -> agent thinks more long term)\n",
    "\n",
    "# TRAINING Hyperparameters\n",
    "RENDER_INTERVAL = 10 # Intervall when the game is rendered\n",
    "TOTAL_EPISODES = 1000000\n",
    "REPLAY_INTERVAL = 10 # Replay every x steps (retrain model) \n",
    "MINI_BATCHES_REPLAY = 32\n",
    "REPLAY_BUFFER_MEMORY = 3000\n",
    "STACKED_FRAMES_SIZE = 4\n",
    "AVERAGE_WINDOW = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Q-Learning Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn(input_shape=(210, 160, 4), ACTIONS_SIZE=5):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(ACTIONS_SIZE, activation='linear')  # Linear activation for Q-values\n",
    "    ])\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',  # Use 'mse' for Q-learning\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stacked Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACKED_FRAMES = deque([np.zeros((STATE_SIZE), dtype=np.int32) for i in range(STACKED_FRAMES_SIZE)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    if is_new_episode:\n",
    "        # clear stack for new episode\n",
    "        stacked_frames = deque([np.zeros((STATE_SIZE), dtype=np.int32) for i in range(STACKED_FRAMES_SIZE)], maxlen=4)\n",
    "        \n",
    "        # Add the same frame 4 times to the deque since its a new episode\n",
    "        stacked_frames.append(state)\n",
    "        stacked_frames.append(state)\n",
    "        stacked_frames.append(state)\n",
    "        stacked_frames.append(state)\n",
    "        \n",
    "        # Stack the frames with numpy (join all 4 frames)\n",
    "        stacked_frames_array = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    elif not is_new_episode:\n",
    "        # append new frame and remove oldest frame\n",
    "        stacked_frames.append(state)\n",
    "\n",
    "        # Stack the frames with numpy (join all 4 frames)\n",
    "        stacked_frames_array = np.stack(stacked_frames, axis=2) \n",
    "\n",
    "    stack_expanded = stacked_frames_array.reshape((1,) + stacked_frames_array.shape)\n",
    "    return stack_expanded, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocess Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(frame):\n",
    "    \n",
    "    # resize frame to fit for cnn\n",
    "    #print(\"before\", frame.shape)\n",
    "    #frame = np.reshape(frame, (210, 160))\n",
    "    #print(frame.shape)\n",
    "    \n",
    "    # normalize pixel values to [-1, 1]\n",
    "    frame = frame / 127.5 - 1.0 \n",
    "    frame = frame.astype(np.float32)\n",
    "\n",
    "    return frame\n",
    "\n",
    "    # TODO: flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        # deque that ther are only max REPLAY_BUFFER_MEMORY items in the list\n",
    "        # deque = remove oldest item\n",
    "        self.buffer = deque(maxlen=REPLAY_BUFFER_MEMORY)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        # add item to buffer\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, MINI_BATCHES_REPLAY) # 16 (MINI_BATCHES_REPLAY) samples to retrain the mdoel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DQN Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.EPSILON = EPSILON\n",
    "        self.model = build_dqn()\n",
    "        self.target_model = build_dqn()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # flattend_array_state = np.expand_dims(state.reshape(-1), axis=0)\n",
    "        # flattend_array_next_state = np.expand_dims(next_state.reshape(-1), axis=0)\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "    \n",
    "    def predict_action(self, stacked_array):\n",
    "        # flatten array for model\n",
    "        #flattend_array = np.expand_dims(stacked_array.reshape(-1), axis=0)\n",
    "\n",
    "        if np.random.rand() <= self.EPSILON:\n",
    "            return random.randrange(ACTIONS_SIZE)\n",
    "        q_values = self.model.predict(stacked_array, verbose=0)\n",
    "\n",
    "        action = np.argmax(q_values[0])\n",
    "        #print(q_values)\n",
    "        #print(q_values[0])\n",
    "        #print(action)\n",
    "        return action\n",
    "    \n",
    "    def replay(self):\n",
    "        print(\"Replay\")\n",
    "        minibatch = self.memory.sample()\n",
    "        i = 0\n",
    "        for state, action, reward, next_state, done in tqdm(minibatch):\n",
    "            i += 1\n",
    "            \n",
    "            # Predict Target Q-Values\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            if done:\n",
    "                # If the episode is done the target Q-value for the taken action is set to the received reward\n",
    "                target[0][action] = reward\n",
    "            elif not done:\n",
    "                # If the episode is not done, the target Q-value for the taken action is updated using the Bellman equation\n",
    "                t = self.target_model.predict(next_state, verbose=0)[0]\n",
    "                target[0][action] = reward + GAMMA * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "                \n",
    "        if self.EPSILON > EPSILON_MIN:\n",
    "            self.EPSILON *= EPSILON_DECAY_RATE\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reward Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(initial_reward:int, distance:int, distance_before:int, total_reward, loss_of_live:bool, lives:int, lives_before):\n",
    "    # add distance to reward going forward \n",
    "    if distance > distance_before:\n",
    "        reward = distance\n",
    "    else:\n",
    "        reward = total_reward\n",
    "\n",
    "    # add reward for moving forward\n",
    "    initial_reward *= 2\n",
    "    reward += initial_reward\n",
    "\n",
    "    # reduce reward when colliding\n",
    "    if lives < lives_before:\n",
    "        print(f\"loss_of_live with {distance} steps forward\")\n",
    "        if reward <= 2:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward -= 2\n",
    "\n",
    "    return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reward Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "average_distance_travelled_per_episode = []\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# REWARD PLOT\n",
    "reward_line, = ax1.plot(rewards_per_episode, label='Reward per Episode')\n",
    "average_line, = ax1.plot(average_rewards, label='Moving Average Reward')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Reward and Moving Average Reward over Time')\n",
    "ax1.legend()\n",
    "\n",
    "# DISTANCE PLOT\n",
    "distance_line, = ax2.plot(average_distance_travelled_per_episode, label='AVG. Distance')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('AVG. Distance Travelled')\n",
    "ax2.set_title('Average Distance Travelled Per Episode')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_of_live with 1 steps forward\n",
      "Distance travelled this run: 1 steps forward & 3 lives left!\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‘\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_of_live with 9 steps forward\n",
      "Distance travelled this run: 9 steps forward & 2 lives left!\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    # clear output of cell for every new episode\n",
    "    clear_output(wait=True)\n",
    "    # create env (human to render game and see actions)\n",
    "    if episode % RENDER_INTERVAL == 0 and episode != 0:\n",
    "        env = gym.make(\"ALE/Frogger-v5\", obs_type=\"grayscale\", render_mode=\"human\", difficulty = 0, mode = 0) \n",
    "    else:\n",
    "        env = gym.make(\"ALE/Frogger-v5\", obs_type=\"grayscale\", render_mode=\"rgb_array\", difficulty = 0, mode = 0) \n",
    "    state = env.reset()[0]\n",
    "    preprocessed_state = preprocess_frames(state)\n",
    "\n",
    "    # reset episode variables\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    is_new_episode = True\n",
    "    distrance_travelled = 0\n",
    "    distance_before = 0\n",
    "    current_lives = 4\n",
    "    lives_before = 4\n",
    "    episode_max_travelled_distance = 0\n",
    "    episode_step_distances = []\n",
    "\n",
    "    # initilize episode stack\n",
    "    stacked_array_state, STACKED_FRAMES = stack_frames(STACKED_FRAMES, preprocessed_state, is_new_episode)\n",
    "    \n",
    "    while not done:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        if is_new_episode and step_count <= 110:\n",
    "            # jsut wait for first 100 steps because of initialization \n",
    "            action = 0\n",
    "            next_state, reward, done, loss_of_live, info = env.step(0)\n",
    "            state = next_state\n",
    "        else:\n",
    "            action = agent.predict_action(stacked_array_state)\n",
    "            next_state, reward, done, loss_of_live, info = env.step(action)\n",
    "            next_state_preprocessed = preprocess_frames(next_state)\n",
    "            is_new_episode = False\n",
    "            \n",
    "            # Do Stacking\n",
    "            stacked_array_next_state, STACKED_FRAMES = stack_frames(STACKED_FRAMES, next_state_preprocessed, is_new_episode)\n",
    "\n",
    "            # updated distance\n",
    "            if action == 1:\n",
    "                distrance_travelled += 1\n",
    "            if action == 4 and distrance_travelled > 0:\n",
    "                distrance_travelled -= 1\n",
    "\n",
    "            #update lives\n",
    "            current_lives = info[\"lives\"]\n",
    "\n",
    "            # update reward\n",
    "            total_reward = calculate_reward(reward, distrance_travelled, distance_before, total_reward, loss_of_live, current_lives, lives_before)\n",
    "            total_reward += reward\n",
    "\n",
    "            # store action infromation in memory\n",
    "            agent.remember(stacked_array_state, action, reward, stacked_array_next_state, done)\n",
    "\n",
    "            # set the next state to the current state\n",
    "            stacked_array_state = stacked_array_next_state\n",
    "\n",
    "            # Do Replay\n",
    "            if step_count % REPLAY_INTERVAL == 0 and not is_new_episode and len(agent.memory.buffer) > MINI_BATCHES_REPLAY:\n",
    "                agent.replay()\n",
    "\n",
    "            if distrance_travelled == 7:\n",
    "                print(\"ðŸ‘‘\")\n",
    "\n",
    "            distance_before = distrance_travelled\n",
    "            \n",
    "            # if frog dies and loses a life\n",
    "            if current_lives < lives_before:\n",
    "                # update max travvellled distance in this episode\n",
    "                if distrance_travelled > episode_max_travelled_distance:\n",
    "                    episode_max_travelled_distance = distrance_travelled\n",
    "                episode_step_distances.append(distrance_travelled)\n",
    "\n",
    "                print(f\"Distance travelled this run: {distrance_travelled} steps forward & {current_lives} lives left!\")\n",
    "                distrance_travelled = 0  \n",
    "                lives_before = current_lives\n",
    "\n",
    "            # if game finished (won or lose)\n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "\n",
    "                rewards_per_episode.append(total_reward)\n",
    "                average_distance_travelled_per_episode.append(np.mean(episode_step_distances))\n",
    "\n",
    "                if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                    moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "                else:\n",
    "                    moving_average = np.mean(rewards_per_episode)\n",
    "                average_rewards.append(moving_average)\n",
    "\n",
    "                step_text = f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Max trav. dist.: {episode_max_travelled_distance}, Epsilon: {agent.EPSILON:.2}\"\n",
    "\n",
    "                # Update REWARD PLOT\n",
    "                reward_line.set_data(range(episode + 1), rewards_per_episode[:episode + 1])\n",
    "                average_line.set_data(range(episode + 1), average_rewards[:episode + 1])\n",
    "                ax1.set_xlim(0, episode + 1)\n",
    "                ax1.set_ylim(0, max(max(rewards_per_episode[:episode + 1]), max(average_rewards[:episode + 1])) + 10)\n",
    "                ax1.set_title(step_text)\n",
    "                \n",
    "                # Update DISTANCE PLOT\n",
    "                distance_line.set_data(range(episode + 1), average_distance_travelled_per_episode[:episode + 1])\n",
    "                ax2.set_xlim(0, episode + 1)\n",
    "                ax2.set_ylim(0, max(average_distance_travelled_per_episode[:episode + 1]) + 10)\n",
    "                \n",
    "                # Redraw the figure\n",
    "                fig.canvas.draw()\n",
    "                fig.canvas.flush_events()\n",
    "                \n",
    "                # Save the figure\n",
    "                fig.savefig(f'/plots/training.png')\n",
    "                \n",
    "                time.sleep(0.1)\n",
    "\n",
    "                print(step_text)\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
