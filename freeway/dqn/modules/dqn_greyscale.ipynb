{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from modules.agents import DQNAgent\n",
    "from modules.logger import EpisodeLogger\n",
    "import modules.rewards as rewards\n",
    "import modules.process as process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Frogger Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [104, 104, 104, ..., 104, 104, 104],\n",
      "       [104, 104, 104, ..., 104, 104, 104],\n",
      "       [104, 104, 104, ..., 104, 104, 104]], dtype=uint8), 0.0, False, False, {'lives': 4, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "State Frame Size: Box(0, 255, (210, 160), uint8)\n",
      "Number Of Actions: 5\n",
      "Possible Actions: \n",
      " [[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "Obervation: \n",
      " [[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [104 104 104 ... 104 104 104]\n",
      " [104 104 104 ... 104 104 104]\n",
      " [104 104 104 ... 104 104 104]]\n",
      "(210, 160)\n"
     ]
    }
   ],
   "source": [
    "# create env with gymnasium (use ram or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Frogger-v5\", # \"ALE/Frogger-ram-v5\" or \"ALE/Frogger-v5\"\n",
    "    obs_type=\"grayscale\", # ram, grescale, rgb\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0, 1, 2]\n",
    "    ) \n",
    "\n",
    "env.reset()\n",
    "print(env.step(0))\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation[0]}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Frogger-v5\", obs_type=\"grayscale\", render_mode=\"rgb_array\", difficulty = 0, mode = 0) \n",
    "\n",
    "# MODEL Hyperparameters\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTIONS_SIZE = env.action_space.n\n",
    "ACTIONS = list(range(0, ACTIONS_SIZE))\n",
    "LEARNING_RATE = 0.001 # Learning Rate (alpha)\n",
    "\n",
    "# AGENT Hyperparameters (epsilon greedy strategy)\n",
    "EPSILON = 0.95\n",
    "EPSILON_MIN = 0.001 # EPSELON value where exploreation stops\n",
    "EPSILON_DECAY_RATE = 0.9995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "GAMMA = 0.975 # Discounting rate (lower -> agent thinks more long term)\n",
    "\n",
    "# TRAINING Hyperparameters\n",
    "RENDER_INTERVAL = 10 # Intervall when the game is rendered\n",
    "TOTAL_EPISODES = 1000\n",
    "REPLAY_INTERVAL = 1 # Replay every x steps (retrain model) \n",
    "MINI_BATCHES_REPLAY = 64\n",
    "REPLAY_BUFFER_MEMORY = 6000\n",
    "STACKED_FRAMES_SIZE = 4\n",
    "AVERAGE_WINDOW = 10\n",
    "\n",
    "STACKED_FRAMES = deque([np.zeros((STATE_SIZE), dtype=np.int32) for i in range(STACKED_FRAMES_SIZE)], maxlen=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reward Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "distance_per_episode = []\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# REWARD PLOT\n",
    "reward_line, = ax1.plot(rewards_per_episode, label='Reward per Episode')\n",
    "average_line, = ax1.plot(average_rewards, label='Moving Average Reward')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Reward and Moving Average Reward over Time')\n",
    "ax1.legend()\n",
    "\n",
    "# DISTANCE PLOT\n",
    "distance_line, = ax2.plot(distance_per_episode, label='AVG. Distance')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('AVG. Distance Travelled')\n",
    "ax2.set_title('Average Distance Travelled Per Episode')\n",
    "ax2.legend()\n",
    "\n",
    "fig.savefig('figures/training.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Over: Distance travelled this run: 2 steps forward & 3 lives left!\n",
      "Episode: 42/1000, Total Reward: -19.0, Moving AVG. Reward: -24.1, Max trav. dist.: 2, Epsilon: 0.72\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 18/64 [00:10<00:27,  1.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Do Replay\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_count \u001b[38;5;241m%\u001b[39m REPLAY_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_new_episode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m MINI_BATCHES_REPLAY:\n\u001b[1;32m---> 67\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m distance_before \u001b[38;5;241m=\u001b[39m distrance_travelled\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m distrance_travelled \u001b[38;5;241m>\u001b[39m episode_max_travelled_distance:\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.vscode\\Projects\\Atari-Frogger-Reinforcement-Learning\\models\\q-learning\\modules\\agents.py:59\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mpredict(next_state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     58\u001b[0m         target[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGAMMA \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(t)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEPSILON \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEPSILON_MIN:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEPSILON \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEPSILON_DECAY_RATE\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\keras\\engine\\training.py:1548\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1541\u001b[0m (\n\u001b[0;32m   1542\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_epoch,\n\u001b[0;32m   1543\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step,\n\u001b[0;32m   1544\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_counters_from_ckpt(\n\u001b[0;32m   1545\u001b[0m     steps_per_epoch_inferred, initial_epoch\n\u001b[0;32m   1546\u001b[0m )\n\u001b[0;32m   1547\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m   1550\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[0;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janmu\\.conda\\envs\\FreewayEnv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3407\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3408\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3409\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3411\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "agent = DQNAgent(REPLAY_BUFFER_MEMORY, MINI_BATCHES_REPLAY, EPSILON, ACTIONS_SIZE, GAMMA, EPSILON_MIN, EPSILON_DECAY_RATE, LEARNING_RATE)\n",
    "logger = EpisodeLogger(log_file_path=\"logs/episode_logs.json\")\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    # create env (human to render game and see actions)\n",
    "    if episode % RENDER_INTERVAL == 0 and episode != 0:\n",
    "        env = gym.make(\"ALE/Frogger-v5\", obs_type=\"grayscale\", render_mode=\"human\", difficulty = 0, mode = 0) \n",
    "    else:\n",
    "        env = gym.make(\"ALE/Frogger-v5\", obs_type=\"grayscale\", render_mode=\"rgb_array\", difficulty = 0, mode = 0) \n",
    "    state = env.reset()[0]\n",
    "    preprocessed_state = process.preprocess_frames(state)\n",
    "\n",
    "    # reset episode variables\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    is_new_episode = True\n",
    "    distrance_travelled = 0\n",
    "    distance_before = 0\n",
    "    current_lives = 4\n",
    "    lives_before = 4\n",
    "    episode_max_travelled_distance = 0\n",
    "    episode_step_distance = 0\n",
    "\n",
    "    # initilize episode stack\n",
    "    stacked_array_state, STACKED_FRAMES = process.stack_frames(STACKED_FRAMES, preprocessed_state, is_new_episode, STATE_SIZE, STACKED_FRAMES_SIZE)\n",
    "    \n",
    "    while not current_lives < lives_before:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        if is_new_episode and step_count <= 110:\n",
    "            # jsut wait for first 100 steps because of initialization \n",
    "            action = 0\n",
    "            next_state, reward, done, loss_of_live, info = env.step(0)\n",
    "            state = next_state\n",
    "        else:\n",
    "            action = agent.predict_action(stacked_array_state)\n",
    "            next_state, reward, done, loss_of_live, info = env.step(action)\n",
    "            next_state_preprocessed = process.preprocess_frames(next_state)\n",
    "            is_new_episode = False\n",
    "            \n",
    "            # Do Stacking\n",
    "            stacked_array_next_state, STACKED_FRAMES = process.stack_frames(STACKED_FRAMES, next_state_preprocessed, is_new_episode, STATE_SIZE, STACKED_FRAMES_SIZE)\n",
    "\n",
    "            # updated distance\n",
    "            if reward == 1:\n",
    "                distrance_travelled += 1\n",
    "\n",
    "            #update lives\n",
    "            current_lives = info[\"lives\"]\n",
    "\n",
    "            # update reward\n",
    "            total_reward = rewards.action_based_reward(total_reward, action, distrance_travelled, distance_before, current_lives, lives_before)\n",
    "\n",
    "            # store action infromation in memory\n",
    "            agent.remember(stacked_array_state, action, reward, stacked_array_next_state, done)\n",
    "\n",
    "            # set the next state to the current state\n",
    "            stacked_array_state = stacked_array_next_state\n",
    "\n",
    "            # Do Replay\n",
    "            if step_count % REPLAY_INTERVAL == 0 and not is_new_episode and len(agent.memory.buffer) > MINI_BATCHES_REPLAY:\n",
    "                agent.replay()\n",
    "\n",
    "            distance_before = distrance_travelled\n",
    "            if distrance_travelled > episode_max_travelled_distance:\n",
    "                episode_max_travelled_distance = distrance_travelled\n",
    "\n",
    "            # if frog crosses the highway or dies\n",
    "            if (distrance_travelled) == 6 or (current_lives < lives_before):\n",
    "                # clear output of cell for every new episode\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                distance_per_episode.append(distrance_travelled)\n",
    "\n",
    "                if distrance_travelled == 6:\n",
    "                    print(f\"ðŸ‘‘: Distance travelled this run: {distrance_travelled} steps forward & {current_lives} lives left!\")\n",
    "                if current_lives < lives_before:\n",
    "                    print(f\"Game Over: Distance travelled this run: {distrance_travelled} steps forward & {current_lives} lives left!\")\n",
    "                    distrance_travelled = 0  \n",
    "                    lives_before = current_lives + 1\n",
    "\n",
    "                agent.update_target_model()\n",
    "\n",
    "                rewards_per_episode.append(total_reward)\n",
    "\n",
    "                if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                    moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "                else:\n",
    "                    moving_average = np.mean(rewards_per_episode)\n",
    "                average_rewards.append(moving_average)\n",
    "\n",
    "                step_text = f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Max trav. dist.: {episode_max_travelled_distance}, Epsilon: {agent.EPSILON:.2}\"\n",
    "\n",
    "                # Update REWARD PLOT\n",
    "                reward_line.set_data(range(episode + 1), rewards_per_episode[:episode + 1])\n",
    "                average_line.set_data(range(episode + 1), average_rewards[:episode + 1])\n",
    "                ax1.set_xlim(0, episode + 1)\n",
    "                ax1.set_title(step_text)\n",
    "                y_min = min(min(rewards_per_episode[:episode + 1]), min(average_rewards[:episode + 1])) - 10\n",
    "                y_max = max(max(rewards_per_episode[:episode + 1]), max(average_rewards[:episode + 1])) + 10\n",
    "                ax1.set_ylim(y_min, y_max)\n",
    "                \n",
    "                # Update DISTANCE PLOT\n",
    "                distance_line.set_data(range(episode + 1), distance_per_episode[:episode + 1])\n",
    "                ax2.set_xlim(0, episode + 1)\n",
    "                y_min_distance = 0\n",
    "                y_max_distance = max(distance_per_episode[:episode + 1]) + 1\n",
    "                ax2.set_ylim(y_min_distance, y_max_distance)\n",
    "                \n",
    "                # Redraw the figure\n",
    "                fig.canvas.draw()\n",
    "                fig.canvas.flush_events()\n",
    "                \n",
    "                # Save the figure\n",
    "                fig.savefig(f'figures/training.png')\n",
    "                \n",
    "                time.sleep(0.1)\n",
    "\n",
    "                print(step_text)\n",
    "                logger.log_episode(episode, total_reward, moving_average, episode_max_travelled_distance, agent.EPSILON, step_count, episode_step_distance, distance_per_episode)\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
