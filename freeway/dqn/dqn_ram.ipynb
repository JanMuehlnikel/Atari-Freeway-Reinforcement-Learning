{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from modules.agents import DQNAgent\n",
    "from modules.logger import EpisodeLogger\n",
    "from modules.logger import save_params\n",
    "import modules.rewards as rewards\n",
    "import modules.processing as processing\n",
    "import modules.figures as figures\n",
    "import modules.networks as networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Freeway Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,   4, 132,   0,  15,  15,   0, 255,  74,  30,  12,   6,   0,\n",
      "         8,   6,   6, 255, 255,   7,   7,  26,   0, 255,  80,  64,  48,\n",
      "        32,  16, 144, 160, 176, 192, 208,   2,   1,   0,   1,   0,   0,\n",
      "         0,   2,   0,   1,  80,  80,  80,  64,  48,  10, 234, 234, 218,\n",
      "       218, 122, 138,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,\n",
      "        80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  26,\n",
      "       216,  68, 136,  36, 130,  74,  18, 220,  66, 189, 247, 122, 247,\n",
      "       122, 247,  80, 247,  80, 247,   0, 247,   0, 247,   0,   0,   0,\n",
      "         0,   0,   0,   0,   1,   1,   1,   2,   3, 156, 158, 158, 159,\n",
      "       159,   0,  16,   5,   1,  80, 255,  87, 246,  75, 244], dtype=uint8), 0.0, False, False, {'lives': 0, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "State Frame Size: Box(0, 255, (128,), uint8)\n",
      "Number Of Actions: 3\n",
      "Possible Actions: \n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Obervation: \n",
      " [  0   4 132   0  14  15   0 255  74  30  12   6   0   8  10   6 255 255\n",
      "   7   7  26   0 255  80  64  48  32  16 144 160 176 192 208   2   1   0\n",
      "   1   0   0   0   2   0   1  80  80  80  64  48  10 234 234 218 218  86\n",
      " 102  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80\n",
      "  80  80  80  80  80  26 216  68 136  36 130  74  18 220  66 189 247  86\n",
      " 247 122 247  80 247  80 247   0 247   0 247   0   0   0   0   0   0   0\n",
      "   1   1   1   2   3 156 158 158 159 159   0  16   5   1  80 255  87 246\n",
      "  75 244]\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# create env with gymnasium (use ram, greyscale or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Freeway-v5\", # \"Enviroment Variant\"\n",
    "    obs_type=\"ram\", # ram, grescale, rgb\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0]\n",
    "    ) \n",
    "\n",
    "env.reset()\n",
    "print(env.step(0))\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation[0]}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMODEL_VERSION = \"v_1\"\\n\\n# GAME PARAMETER\\nEVIROMENT_VARIANT = \"ALE/Freeway-v5\"\\nDIFFICULTY = 0\\nMODE = 0\\n\\nenv = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty = 0, mode = 0) \\n\\n# MODEL Hyperparameters\\nSTATE_SIZE = env.observation_space.shape[0]\\nACTIONS_SIZE = env.action_space.n\\nACTIONS = list(range(0, ACTIONS_SIZE))\\nLEARNING_RATE = 0.001 # Learning Rate (alpha)\\n\\n# AGENT Hyperparameters (epsilon greedy strategy)\\nEPSILON = 0.99\\nEPSILON_MIN = 0.01 # EPSELON value where exploreation stops\\nEPSILON_DECAY_RATE = 0.9995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\\nGAMMA = 0.99 # Discounting rate (lower -> agent thinks more long term)\\n\\n# TRAINING Hyperparameters\\nRENDER_INTERVAL = 100 # Intervall when the game is rendered\\nTOTAL_EPISODES = 1000\\nREPLAY_INTERVAL = 4 # Replay every x steps (retrain model) \\nMINI_BATCHES_REPLAY = 16\\nREPLAY_BUFFER_MEMORY = 200000\\nMINIMUM_REPLAY_HISTORY = 10000\\nAVERAGE_WINDOW = 10\\nUPDATE_TARGET_MODEL_FREQUENCY = 5 # Intevall of episodes the target model is updated\\nMODEL_SAVE_INTERVALL = 10 # Intervall in which the target model is saved\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODEL_VERSION = \"v_1\"\n",
    "\n",
    "# GAME PARAMETER\n",
    "EVIROMENT_VARIANT = \"ALE/Freeway-v5\"\n",
    "DIFFICULTY = 0\n",
    "MODE = 0\n",
    "\n",
    "env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty = 0, mode = 0) \n",
    "\n",
    "# MODEL Hyperparameters\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTIONS_SIZE = env.action_space.n\n",
    "ACTIONS = list(range(0, ACTIONS_SIZE))\n",
    "LEARNING_RATE = 0.001 # Learning Rate (alpha)\n",
    "\n",
    "# AGENT Hyperparameters (epsilon greedy strategy)\n",
    "EPSILON = 0.99\n",
    "EPSILON_MIN = 0.01 # EPSELON value where exploreation stops\n",
    "EPSILON_DECAY_RATE = 0.9995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "GAMMA = 0.99 # Discounting rate (lower -> agent thinks more long term)\n",
    "\n",
    "# TRAINING Hyperparameters\n",
    "RENDER_INTERVAL = 100 # Intervall when the game is rendered\n",
    "TOTAL_EPISODES = 1000\n",
    "REPLAY_INTERVAL = 4 # Replay every x steps (retrain model) \n",
    "MINI_BATCHES_REPLAY = 16\n",
    "REPLAY_BUFFER_MEMORY = 200000\n",
    "MINIMUM_REPLAY_HISTORY = 10000\n",
    "AVERAGE_WINDOW = 10\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = 5 # Intevall of episodes the target model is updated\n",
    "MODEL_SAVE_INTERVALL = 10 # Intervall in which the target model is saved\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change parameters here\n",
    "params = {\n",
    "    \"MODEL_VERSION\": \"v_1\",\n",
    "    \"EVIROMENT_VARIANT\": \"ALE/Freeway-v5\",\n",
    "    \"DIFFICULTY\": 0,\n",
    "    \"MODE\": 0,\n",
    "    \"obs_type\": \"ram\",\n",
    "    \"STATE_SIZE\": env.observation_space.shape[0], \n",
    "    \"ACTIONS_SIZE\": env.action_space.n,\n",
    "    \"ACTIONS\": list(range(0, env.action_space.n)),\n",
    "    \"LEARNING_RATE\": 0.001, # Learning Rate (alpha)\n",
    "    \"EPSILON\": 0.99,\n",
    "    \"EPSILON_MIN\": 0.01, # EPSELON value where exploreation stops\n",
    "    \"EPSILON_DECAY_RATE\": 0.9995, # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "    \"GAMMA\": 0.99, # Discounting rate (lower -> agent thinks more long term)\n",
    "    \"RENDER_INTERVAL\": 100, # Intervall when the game is rendered\n",
    "    \"TOTAL_EPISODES\": 1000,\n",
    "    \"REPLAY_INTERVAL\": 4, # Replay every x steps (retrain model) \n",
    "    \"MINI_BATCHES_REPLAY\": 16,\n",
    "    \"REPLAY_BUFFER_MEMORY\": 200000,\n",
    "    \"MINIMUM_REPLAY_HISTORY\": 10000,\n",
    "    \"AVERAGE_WINDOW\": 10,\n",
    "    \"UPDATE_TARGET_MODEL_FREQUENCY\": 5, # Intevall of episodes the target model is updated\n",
    "    \"MODEL_SAVE_INTERVALL\": 10 # Intervall in which the target model is saved\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = params[\"MODEL_VERSION\"]\n",
    "EVIROMENT_VARIANT = params[\"EVIROMENT_VARIANT\"]\n",
    "obs_type = params[\"obs_type\"]\n",
    "DIFFICULTY = params[\"DIFFICULTY\"]\n",
    "MODE = params[\"MODE\"]\n",
    "STATE_SIZE = params[\"STATE_SIZE\"]\n",
    "ACTIONS_SIZE = params[\"ACTIONS_SIZE\"]\n",
    "ACTIONS = params[\"ACTIONS\"]\n",
    "LEARNING_RATE = params[\"LEARNING_RATE\"]\n",
    "EPSILON = params[\"EPSILON\"]\n",
    "EPSILON_MIN = params[\"EPSILON_MIN\"]\n",
    "EPSILON_DECAY_RATE = params[\"EPSILON_DECAY_RATE\"]\n",
    "GAMMA = params[\"GAMMA\"]\n",
    "RENDER_INTERVAL = params[\"RENDER_INTERVAL\"]\n",
    "TOTAL_EPISODES = params[\"TOTAL_EPISODES\"]\n",
    "REPLAY_INTERVAL = params[\"REPLAY_INTERVAL\"]\n",
    "MINI_BATCHES_REPLAY = params[\"MINI_BATCHES_REPLAY\"]\n",
    "REPLAY_BUFFER_MEMORY = params[\"REPLAY_BUFFER_MEMORY\"]\n",
    "MINIMUM_REPLAY_HISTORY = params[\"MINIMUM_REPLAY_HISTORY\"]\n",
    "AVERAGE_WINDOW = params[\"AVERAGE_WINDOW\"]\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = params[\"UPDATE_TARGET_MODEL_FREQUENCY\"]\n",
    "MODEL_SAVE_INTERVALL = params[\"MODEL_SAVE_INTERVALL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(params, f\"parameters/{MODEL_VERSION}/\", MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Agent and Logger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(REPLAY_BUFFER_MEMORY, MINI_BATCHES_REPLAY, EPSILON, ACTIONS_SIZE, GAMMA, EPSILON_MIN, EPSILON_DECAY_RATE, LEARNING_RATE)\n",
    "logger = EpisodeLogger(log_files_dir=f\"logs/{MODEL_VERSION}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fill Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=DIFFICULTY, mode=MODE) \n",
    "\n",
    "state = env.reset()[0]\n",
    "preprocessed_state = processing.preprocess_ram(state)\n",
    "total_reward = 0\n",
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "distance_per_episode = []\n",
    "\n",
    "with tqdm(total=MINIMUM_REPLAY_HISTORY) as pbar:\n",
    "    while len(agent.memory.buffer) < MINIMUM_REPLAY_HISTORY:\n",
    "        # take random action action\n",
    "        action = random.randint(0, ACTIONS_SIZE - 1)\n",
    "        next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "        y_pos = env.ale.getRAM()[14] // 3\n",
    "        crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "        next_state_preprocessed = processing.preprocess_ram(next_state)\n",
    "\n",
    "        # update reward\n",
    "        total_reward, gained_reward = rewards.action_based_reward(total_reward, crashed, action, y_pos, game_reward)\n",
    "\n",
    "        # store action infromation in memory\n",
    "        agent.remember(preprocessed_state, next_state_preprocessed, action, gained_reward, crashed)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        preprocessed_state = next_state_preprocessed\n",
    "        pbar.update(len(agent.memory.buffer) - pbar.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    print(f\"EPISODE: {episode}\")\n",
    "    # create env (human to render game and see actions)\n",
    "    if episode % RENDER_INTERVAL == 0 and episode != 0:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", render_mode=\"human\", difficulty=DIFFICULTY, mode=MODE) \n",
    "    else:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=DIFFICULTY, mode=MODE) \n",
    "        \n",
    "    state = env.reset()[0]\n",
    "    preprocessed_state = processing.preprocess_ram(state)\n",
    "\n",
    "    # reset episode variables\n",
    "    step_count = 0\n",
    "    distance_before = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # initilize episode stack\n",
    "    #stacked_array_state, STACKED_FRAMES = process.stack_frames(STACKED_FRAMES, preprocessed_state, is_new_episode, STATE_SIZE, STACKED_FRAMES_SIZE)\n",
    "    \n",
    "    while True:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        action = agent.predict_action(preprocessed_state)\n",
    "        next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "        y_pos = env.ale.getRAM()[14] // 3\n",
    "        crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "        next_state_preprocessed = processing.preprocess_ram(next_state)\n",
    "\n",
    "        # update reward\n",
    "        total_reward, gained_reward = rewards.action_based_reward(total_reward, crashed, action, y_pos, game_reward)\n",
    "\n",
    "        # store action infromation in memory\n",
    "        agent.remember(preprocessed_state, next_state_preprocessed, action, gained_reward, crashed)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        preprocessed_state = next_state_preprocessed\n",
    "\n",
    "        # Do Replay\n",
    "        agent.replay()\n",
    "\n",
    "        # END EPISODE IF CHICKEN COLLIDES\n",
    "        if crashed == 1 or game_reward == 1:\n",
    "            # clear output of cell for every new episode\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # update ntework\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            rewards_per_episode.append(total_reward)\n",
    "            distance_per_episode.append(y_pos)\n",
    "\n",
    "            if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "            else:\n",
    "                moving_average = np.mean(rewards_per_episode)\n",
    "            average_rewards.append(moving_average)\n",
    "\n",
    "            step_text = f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Distance: {y_pos}, Epsilon: {agent.EPSILON:.2}\"\n",
    "            \n",
    "            figures.reward_plot(rewards_per_episode, average_rewards, dir=f\"figures/{MODEL_VERSION}/\")\n",
    "            figures.distance_plot(distance_per_episode, dir=f\"figures/{MODEL_VERSION}/\")\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            print(step_text)\n",
    "            if episode % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            logger.log_episode(total_reward, moving_average, agent.EPSILON, y_pos)\n",
    "            networks.save_model(agent.target_model, episode, MODEL_SAVE_INTERVALL, dir=f\"models/{MODEL_VERSION}/\")\n",
    "            break\n",
    "\n",
    "        is_new_episode = False\n",
    "        distance_before = y_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
