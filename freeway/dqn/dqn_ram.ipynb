{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from modules.agents import DQNAgent\n",
    "from modules.logger import EpisodeLogger\n",
    "import modules.rewards as rewards\n",
    "import modules.processing as processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Freeway Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,   4, 132,   0,  15,  15,   0, 255,  74,  30,  12,   6,   0,\n",
      "         8,   6,   6, 255, 255,   7,   7,  26,   0, 255,  80,  64,  48,\n",
      "        32,  16, 144, 160, 176, 192, 208,   2,   1,   0,   1,   0,   0,\n",
      "         0,   2,   0,   1,  80,  80,  80,  64,  48,  10, 234, 234, 218,\n",
      "       218, 122, 138,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,\n",
      "        80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  26,\n",
      "       216,  68, 136,  36, 130,  74,  18, 220,  66, 189, 247, 122, 247,\n",
      "       122, 247,  80, 247,  80, 247,   0, 247,   0, 247,   0,   0,   0,\n",
      "         0,   0,   0,   0,   1,   1,   1,   2,   3, 156, 158, 158, 159,\n",
      "       159,   0,  16,   5,   1,  80, 255,  87, 246,  75, 244], dtype=uint8), 0.0, False, False, {'lives': 0, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "State Frame Size: Box(0, 255, (128,), uint8)\n",
      "Number Of Actions: 3\n",
      "Possible Actions: \n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Obervation: \n",
      " [  0   4 132   0  14  15   0 255  74  30  12   6   0   8  10   6 255 255\n",
      "   7   7  26   0 255  80  64  48  32  16 144 160 176 192 208   2   1   0\n",
      "   1   0   0   0   2   0   1  80  80  80  64  48  10 234 234 218 218  86\n",
      " 102  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80\n",
      "  80  80  80  80  80  26 216  68 136  36 130  74  18 220  66 189 247  86\n",
      " 247 122 247  80 247  80 247   0 247   0 247   0   0   0   0   0   0   0\n",
      "   1   1   1   2   3 156 158 158 159 159   0  16   5   1  80 255  87 246\n",
      "  75 244]\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# create env with gymnasium (use ram, greyscale or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Freeway-v5\", # \"Enviroment Variant\"\n",
    "    obs_type=\"ram\", # ram, grescale, rgb\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0]\n",
    "    ) \n",
    "\n",
    "env.reset()\n",
    "print(env.step(0))\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation[0]}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVIROMENT_VARIANT = \"ALE/Freeway-v5\"\n",
    "\n",
    "env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty = 0, mode = 0) \n",
    "\n",
    "# MODEL Hyperparameters\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTIONS_SIZE = env.action_space.n\n",
    "ACTIONS = list(range(0, ACTIONS_SIZE))\n",
    "LEARNING_RATE = 0.001 # Learning Rate (alpha)\n",
    "\n",
    "# AGENT Hyperparameters (epsilon greedy strategy)\n",
    "EPSILON = 0.99\n",
    "EPSILON_MIN = 0.001 # EPSELON value where exploreation stops\n",
    "EPSILON_DECAY_RATE = 0.9995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "GAMMA = 0.975 # Discounting rate (lower -> agent thinks more long term)\n",
    "\n",
    "# TRAINING Hyperparameters\n",
    "RENDER_INTERVAL = 10 # Intervall when the game is rendered\n",
    "TOTAL_EPISODES = 1000\n",
    "REPLAY_INTERVAL = 4 # Replay every x steps (retrain model) \n",
    "MINI_BATCHES_REPLAY = 32\n",
    "REPLAY_BUFFER_MEMORY = 6000\n",
    "STACKED_FRAMES_SIZE = 4\n",
    "AVERAGE_WINDOW = 10\n",
    "\n",
    "STACKED_FRAMES = deque([np.zeros((STATE_SIZE), dtype=np.int32) for i in range(STACKED_FRAMES_SIZE)], maxlen=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reward Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "distance_per_episode = []\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# REWARD PLOT\n",
    "reward_line, = ax1.plot(rewards_per_episode, label='Reward per Episode')\n",
    "average_line, = ax1.plot(average_rewards, label='Moving Average Reward')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Reward and Moving Average Reward over Time')\n",
    "ax1.legend()\n",
    "\n",
    "# DISTANCE PLOT\n",
    "distance_line, = ax2.plot(distance_per_episode, label='Distance')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Distance')\n",
    "ax2.set_title('Distance Travelled Per Episode')\n",
    "ax2.legend()\n",
    "\n",
    "fig.savefig('figures/training.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3/1000, Total Reward: -33.0, Moving AVG. Reward: -38.375, Distance: 31, Epsilon: 0.77\n",
      "EPISODE: 4\n",
      "[2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[3 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[4 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[6 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:04<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[8 0 0 0 0 0 7 0 0 0 0 0]\n",
      "[8 0 0 0 0 0 9 0 0 0 0 0]\n",
      "[10  0  0  0  0  0 10  0  0  0  0  0]\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:04<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  0  0  0  0  0 11  0  0  0  0  0]\n",
      "[10  0  0  0  0  6 13  0  0  0  0  0]\n",
      "[11  0  0  0  0  7 14  0  0  0  0  0]\n",
      "[11  0  0  0  0  8 15  0  0  0  0  0]\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:04<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  0  0  0  0  8 17  0  0  0  0  0]\n",
      "[10  0  0  0  0  9 18  0  0  0  0  0]\n",
      "[11  0  0  0  6 10 19  0  0  0  0  0]\n",
      "[11  0  0  0  7 10 21  0  0  0  0  0]\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:04<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  0  0  0  7 11 22  0  0  0  0  0]\n",
      "[10  0  0  0  8 12 23  0  0  0  0  0]\n",
      "[ 8  0  0  0  8 12 25  0  0  0  0  0]\n",
      "[ 7  0  0  6  9 13 26 26  0  0  0  0]\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:04<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  0  0  7  9 14  0 25  0  0  0  0]\n",
      "[ 8  0  0  7  9 14  0 24  0  0  0  0]\n",
      "[10  0  0  7 10 15  0 22  0  0  0  0]\n",
      "[10  0  0  8 10 16  0 21  0  0  0  0]\n",
      "Replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 8/32 [00:01<00:03,  6.04it/s]"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "agent = DQNAgent(REPLAY_BUFFER_MEMORY, MINI_BATCHES_REPLAY, EPSILON, ACTIONS_SIZE, GAMMA, EPSILON_MIN, EPSILON_DECAY_RATE, LEARNING_RATE)\n",
    "logger = EpisodeLogger(log_file_path=\"logs/episode_logs.json\")\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    print(f\"EPISODE: {episode}\")\n",
    "    # create env (human to render game and see actions)\n",
    "    if episode % RENDER_INTERVAL == 0 and episode != 0:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", render_mode=\"human\", difficulty=0, mode=0) \n",
    "    else:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=0, mode=0) \n",
    "        \n",
    "    state = env.reset()[0]\n",
    "    preprocessed_state = processing.preprocess_ram(state)\n",
    "\n",
    "    # reset episode variables\n",
    "    step_count = 0\n",
    "    max_distance_episode = 0\n",
    "    distance_before = 0\n",
    "    is_new_episode = True\n",
    "    total_reward = 0\n",
    "\n",
    "    # initilize episode stack\n",
    "    #stacked_array_state, STACKED_FRAMES = process.stack_frames(STACKED_FRAMES, preprocessed_state, is_new_episode, STATE_SIZE, STACKED_FRAMES_SIZE)\n",
    "    \n",
    "    while True:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        action = agent.predict_action(preprocessed_state)\n",
    "        next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "        y_pos = env.ale.getRAM()[14]\n",
    "        crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "        next_state_preprocessed = processing.preprocess_ram(next_state)\n",
    "\n",
    "        # Do Stacking\n",
    "        #stacked_array_next_state, STACKED_FRAMES = process.stack_frames(STACKED_FRAMES, next_state_preprocessed, is_new_episode, STATE_SIZE, STACKED_FRAMES_SIZE)\n",
    "\n",
    "        # updated distance\n",
    "        if y_pos > max_distance_episode:\n",
    "            max_distance_episode = y_pos\n",
    "\n",
    "        # update reward\n",
    "        total_reward = rewards.action_based_reward(total_reward, crashed, action, y_pos, max_distance_episode)\n",
    "\n",
    "        # store action infromation in memory\n",
    "        agent.remember(preprocessed_state, action, game_reward, next_state_preprocessed, game_done)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        preprocessed_state = next_state_preprocessed\n",
    "\n",
    "        # Do Replay\n",
    "        if step_count % REPLAY_INTERVAL == 0 and len(agent.memory.buffer) > MINI_BATCHES_REPLAY:\n",
    "            agent.replay()\n",
    "\n",
    "        # END EPISODE IF CHICKEN COLLIDES\n",
    "        if crashed == 1 or y_pos >= 175:\n",
    "            # clear output of cell for every new episode\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # update ntework\n",
    "            agent.update_target_model()\n",
    "\n",
    "            rewards_per_episode.append(total_reward)\n",
    "            distance_per_episode.append(y_pos)\n",
    "\n",
    "            if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "            else:\n",
    "                moving_average = np.mean(rewards_per_episode)\n",
    "            average_rewards.append(moving_average)\n",
    "\n",
    "            step_text = f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Distance: {y_pos}, Epsilon: {agent.EPSILON:.2}\"\n",
    "\n",
    "            # Update REWARD PLOT\n",
    "            reward_line.set_data(range(episode + 1), rewards_per_episode[:episode + 1])\n",
    "            average_line.set_data(range(episode + 1), average_rewards[:episode + 1])\n",
    "            ax1.set_xlim(0, episode + 1)\n",
    "            ax1.set_title(step_text)\n",
    "            y_min = min(min(rewards_per_episode[:episode + 1]), min(average_rewards[:episode + 1])) - 10\n",
    "            y_max = max(max(rewards_per_episode[:episode + 1]), max(average_rewards[:episode + 1])) + 10\n",
    "            ax1.set_ylim(y_min, y_max)\n",
    "            \n",
    "            # Update DISTANCE PLOT\n",
    "            distance_line.set_data(range(episode + 1), distance_per_episode[:episode + 1])\n",
    "            ax2.set_xlim(0, episode + 1)\n",
    "            y_min_distance = 0\n",
    "            y_max_distance = max(distance_per_episode[:episode + 1]) + 1\n",
    "            ax2.set_ylim(y_min_distance, y_max_distance)\n",
    "            \n",
    "            # Redraw the figure\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "            \n",
    "            # Save the figure\n",
    "            fig.savefig(f'figures/training.png')\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "\n",
    "            print(step_text)\n",
    "            #logger.log_episode(episode, total_reward, moving_average, agent.EPSILON, step_count, distance_per_episode)\n",
    "            break\n",
    "\n",
    "        is_new_episode = False\n",
    "        distance_before = y_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
