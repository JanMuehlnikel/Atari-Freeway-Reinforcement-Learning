{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from modules.agents import DQNAgent\n",
    "from modules.logger import EpisodeLogger\n",
    "from modules.logger import save_params\n",
    "import modules.rewards as dqn_rewards\n",
    "import modules.processing as dqn_processing\n",
    "import modules.figures as figures\n",
    "import modules.networks as networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Freeway Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create env with gymnasium (use ram, greyscale or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Freeway-v5\", # \"Enviroment Variant\"\n",
    "    obs_type=\"ram\", # ram, grescale, rgb\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0]\n",
    "    ) \n",
    "\n",
    "env.reset()\n",
    "print(env.step(0))\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation[0]}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change parameters here\n",
    "params = {\n",
    "    \"MODEL_VERSION\": \"v_40.1\",\n",
    "    \"EVIROMENT_VARIANT\": \"ALE/Freeway-v5\",\n",
    "    \"DIFFICULTY\": 0,\n",
    "    \"MODE\": \"0-7\",\n",
    "    \"obs_type\": \"ram\",\n",
    "    \"STATE_SIZE\": env.observation_space.shape[0], \n",
    "    \"ACTIONS_SIZE\": env.action_space.n,\n",
    "    \"ACTIONS\": list(range(0, env.action_space.n)),\n",
    "    \"LEARNING_RATE\": 0.00025, # Learning Rate (alpha)\n",
    "    \"EPSILON\": 0.99,\n",
    "    \"EPSILON_MIN\": 0.05, # EPSELON value where exploreation stops\n",
    "    \"EPSILON_DECAY_RATE\": 0.9995, # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "    \"GAMMA\": 0.975, # Discounting rate (lower -> agent thinks more long term)\n",
    "    \"RENDER_INTERVAL\": 100, # Intervall when the game is rendered\n",
    "    \"TOTAL_EPISODES\": 1000, \n",
    "    \"REPLAY_INTERVAL\": 4, # Replay every x steps (retrain model) \n",
    "    \"MINI_BATCHES_REPLAY\": 32,\n",
    "    \"REPLAY_BUFFER_MEMORY\": 500000,\n",
    "    \"MINIMUM_REPLAY_HISTORY\": 20000,\n",
    "    \"AVERAGE_WINDOW\": 10,\n",
    "    \"UPDATE_TARGET_MODEL_FREQUENCY\": 5, # Intevall of episodes the target model is updated\n",
    "    \"MODEL_SAVE_INTERVALL\": 10 # Intervall in which the target model is saved\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = params[\"MODEL_VERSION\"]\n",
    "EVIROMENT_VARIANT = params[\"EVIROMENT_VARIANT\"]\n",
    "obs_type = params[\"obs_type\"]\n",
    "DIFFICULTY = params[\"DIFFICULTY\"]\n",
    "MODE = params[\"MODE\"]\n",
    "STATE_SIZE = params[\"STATE_SIZE\"]\n",
    "ACTIONS_SIZE = params[\"ACTIONS_SIZE\"]\n",
    "ACTIONS = params[\"ACTIONS\"]\n",
    "LEARNING_RATE = params[\"LEARNING_RATE\"]\n",
    "EPSILON = params[\"EPSILON\"]\n",
    "EPSILON_MIN = params[\"EPSILON_MIN\"]\n",
    "EPSILON_DECAY_RATE = params[\"EPSILON_DECAY_RATE\"]\n",
    "GAMMA = params[\"GAMMA\"]\n",
    "RENDER_INTERVAL = params[\"RENDER_INTERVAL\"]\n",
    "TOTAL_EPISODES = params[\"TOTAL_EPISODES\"]\n",
    "REPLAY_INTERVAL = params[\"REPLAY_INTERVAL\"]\n",
    "MINI_BATCHES_REPLAY = params[\"MINI_BATCHES_REPLAY\"]\n",
    "REPLAY_BUFFER_MEMORY = params[\"REPLAY_BUFFER_MEMORY\"]\n",
    "MINIMUM_REPLAY_HISTORY = params[\"MINIMUM_REPLAY_HISTORY\"]\n",
    "AVERAGE_WINDOW = params[\"AVERAGE_WINDOW\"]\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = params[\"UPDATE_TARGET_MODEL_FREQUENCY\"]\n",
    "MODEL_SAVE_INTERVALL = params[\"MODEL_SAVE_INTERVALL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(params, f\"parameters/{MODEL_VERSION}/\", MODEL_VERSION)\n",
    "dqn_rewards.save_rewards(MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Agent and Logger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(REPLAY_BUFFER_MEMORY, MINI_BATCHES_REPLAY, EPSILON, ACTIONS_SIZE, GAMMA, EPSILON_MIN, EPSILON_DECAY_RATE, LEARNING_RATE)\n",
    "logger = EpisodeLogger(log_files_dir=f\"logs/{MODEL_VERSION}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fill Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "distance_per_episode = []\n",
    "max_distances = []\n",
    "times_per_episode = []\n",
    "\n",
    "with tqdm(total=MINIMUM_REPLAY_HISTORY) as pbar:\n",
    "    while len(agent.memory.buffer) < MINIMUM_REPLAY_HISTORY:\n",
    "        MODE = random.randint(0, 7)\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=DIFFICULTY, mode=MODE) \n",
    "\n",
    "        state = env.reset()[0]\n",
    "        preprocessed_state = dqn_processing.preprocess_ram(state)\n",
    "        total_reward = 0\n",
    "        y_pos_prev = 0\n",
    "        crashed = 0\n",
    "        game_reward = 0\n",
    "\n",
    "        while (crashed == 0 or game_reward == 0) and (len(agent.memory.buffer) < MINIMUM_REPLAY_HISTORY):\n",
    "            # take random action action\n",
    "            action = random.randint(0, ACTIONS_SIZE - 1)\n",
    "            next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "            y_pos = env.ale.getRAM()[14] // 3\n",
    "            crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "            next_state_preprocessed = dqn_processing.preprocess_ram(next_state)\n",
    "\n",
    "            # update reward\n",
    "            total_reward, gained_reward = dqn_rewards.action_based_reward(total_reward, crashed, action, y_pos, y_pos_prev, game_reward)\n",
    "\n",
    "            # store action infromation in memory\n",
    "            agent.remember(preprocessed_state, next_state_preprocessed, action, gained_reward, crashed)\n",
    "\n",
    "            # set the next state to the current state\n",
    "            preprocessed_state = next_state_preprocessed\n",
    "            pbar.update(len(agent.memory.buffer) - pbar.n)\n",
    "            y_pos_prev = y_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(TOTAL_EPISODES):\n",
    "    print(f\"EPISODE: {episode}\")\n",
    "    # create env (human to render game and see actions)\n",
    "    MODE = random.randint(0, 7) # random mode for no static env\n",
    "\n",
    "    if episode % RENDER_INTERVAL == 0 and episode != 0:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", render_mode=\"human\", difficulty=DIFFICULTY, mode=MODE) \n",
    "    else:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=DIFFICULTY, mode=MODE) \n",
    "        \n",
    "    state = env.reset()[0]\n",
    "    preprocessed_state = dqn_processing.preprocess_ram(state)\n",
    "\n",
    "    # reset episode variables\n",
    "    step_count = 0\n",
    "    prev_y_pos = 0\n",
    "    total_reward = 0\n",
    "    max_distance_episode = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        action = agent.predict_action(preprocessed_state)\n",
    "        next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "        y_pos = env.ale.getRAM()[14] // 3\n",
    "        crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "        if y_pos > max_distance_episode:\n",
    "            max_distance_episode = y_pos\n",
    "\n",
    "        next_state_preprocessed = dqn_processing.preprocess_ram(next_state)\n",
    "\n",
    "        # update reward\n",
    "        if game_reward == 1:\n",
    "            total_reward, gained_reward = dqn_rewards.action_based_reward(total_reward, 0, action, 58, prev_y_pos, game_reward)\n",
    "        else:\n",
    "            total_reward, gained_reward = dqn_rewards.action_based_reward(total_reward, crashed, action, y_pos, prev_y_pos, game_reward)\n",
    "\n",
    "        # store action infromation in memory\n",
    "        agent.remember(preprocessed_state, next_state_preprocessed, action, gained_reward, crashed)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        preprocessed_state = next_state_preprocessed\n",
    "\n",
    "        # Do Replay\n",
    "        agent.replay()\n",
    "\n",
    "        # END EPISODE IF CHICKEN COLLIDES\n",
    "        if crashed == 1 or game_reward == 1:\n",
    "            episode_duration = time.time() - start_time\n",
    "            times_per_episode.append(episode_duration)\n",
    "            # clear output of cell for every new episode\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # update ntework\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            rewards_per_episode.append(total_reward)\n",
    "            # if won y_pos is resetted\n",
    "            if game_reward == 1:\n",
    "                distance_per_episode.append(58)\n",
    "            else:\n",
    "                distance_per_episode.append(y_pos)\n",
    "            max_distances.append(max_distance_episode)\n",
    "\n",
    "            if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "            else:\n",
    "                moving_average = np.mean(rewards_per_episode)\n",
    "            average_rewards.append(moving_average)\n",
    "\n",
    "            step_text = f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Distance: {y_pos}, Epsilon: {agent.EPSILON:.2}\"\n",
    "            \n",
    "            figures.reward_plot(rewards_per_episode, average_rewards, dir=f\"figures/{MODEL_VERSION}/\")\n",
    "            figures.distance_plot(distance_per_episode, dir=f\"figures/{MODEL_VERSION}/\")\n",
    "            figures.max_distance_plot(max_distances, dir=f\"figures/{MODEL_VERSION}/\")\n",
    "            figures.time_plot(times_per_episode, dir=f\"figures/{MODEL_VERSION}/\")\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            print(step_text)\n",
    "            if episode % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            logger.log_episode(total_reward, moving_average, agent.EPSILON, y_pos)\n",
    "            networks.save_model(agent.target_model, episode, MODEL_SAVE_INTERVALL, dir=f\"models/{MODEL_VERSION}/\")\n",
    "            break\n",
    "\n",
    "        is_new_episode = False\n",
    "        prev_y_pos = y_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
