{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from modules.agents import DQNAgent\n",
    "from modules.logger import EpisodeLogger\n",
    "import modules.rewards as rewards\n",
    "import modules.processing as processing\n",
    "import modules.figures as figures\n",
    "import modules.networks as networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    print(physical_devices)\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Freeway Enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create env with gymnasium (use ram, greyscale or rgb state)\n",
    "env = gym.make(\n",
    "    \"ALE/Freeway-v5\", # \"Enviroment Variant\"\n",
    "    obs_type=\"ram\", # ram, grescale, rgb\n",
    "    render_mode=\"rgb_array\", # rgb_array or human\n",
    "    difficulty = 0, # [0, 1]\n",
    "    mode = 0 # [0]\n",
    "    ) \n",
    "\n",
    "env.reset()\n",
    "print(env.step(0))\n",
    "print(f\"State Frame Size: {env.observation_space}\")\n",
    "print(f\"Number Of Actions: {env.action_space.n}\")\n",
    "\n",
    "actions_space = possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(f\"Possible Actions: \\n {actions_space}\")\n",
    "\n",
    "env.reset()\n",
    "observation = env.step(1)\n",
    "print(f\"Obervation: \\n {observation[0]}\")\n",
    "print(observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAME PARAMETER\n",
    "EVIROMENT_VARIANT = \"ALE/Freeway-v5\"\n",
    "DIFFICULTY = 0\n",
    "MODE = 0\n",
    "\n",
    "env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty = 0, mode = 0) \n",
    "\n",
    "# MODEL Hyperparameters\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTIONS_SIZE = env.action_space.n\n",
    "ACTIONS = list(range(0, ACTIONS_SIZE))\n",
    "LEARNING_RATE = 0.001 # Learning Rate (alpha)\n",
    "\n",
    "# AGENT Hyperparameters (epsilon greedy strategy)\n",
    "EPSILON = 0.99\n",
    "EPSILON_MIN = 0.01 # EPSELON value where exploreation stops\n",
    "EPSILON_DECAY_RATE = 0.9995 # the higher the longer the exploreation takes (Linear Decay: EPSELON * DECAY_RATE)\n",
    "GAMMA = 0.99 # Discounting rate (lower -> agent thinks more long term)\n",
    "\n",
    "# TRAINING Hyperparameters\n",
    "RENDER_INTERVAL = 100 # Intervall when the game is rendered\n",
    "TOTAL_EPISODES = 1000\n",
    "REPLAY_INTERVAL = 4 # Replay every x steps (retrain model) \n",
    "MINI_BATCHES_REPLAY = 16\n",
    "REPLAY_BUFFER_MEMORY = 200000\n",
    "MINIMUM_REPLAY_HISTORY = 10000\n",
    "AVERAGE_WINDOW = 10\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = 5 # Intevall of episodes the target model is updated\n",
    "MODEL_SAVE_INTERVALL = 10 # Intervall in which the target model is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Agent and Logger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(REPLAY_BUFFER_MEMORY, MINI_BATCHES_REPLAY, EPSILON, ACTIONS_SIZE, GAMMA, EPSILON_MIN, EPSILON_DECAY_RATE, LEARNING_RATE)\n",
    "logger = EpisodeLogger(log_files_dir=\"logs/episode_logs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fill Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=DIFFICULTY, mode=MODE) \n",
    "\n",
    "state = env.reset()[0]\n",
    "preprocessed_state = processing.preprocess_ram(state)\n",
    "total_reward = 0\n",
    "average_rewards = []\n",
    "rewards_per_episode = []\n",
    "distance_per_episode = []\n",
    "\n",
    "with tqdm(total=MINIMUM_REPLAY_HISTORY) as pbar:\n",
    "    while len(agent.memory.buffer) < MINIMUM_REPLAY_HISTORY:\n",
    "        # take random action action\n",
    "        action = random.randint(0, ACTIONS_SIZE - 1)\n",
    "        next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "        y_pos = env.ale.getRAM()[14] // 3\n",
    "        crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "        next_state_preprocessed = processing.preprocess_ram(next_state)\n",
    "\n",
    "        # update reward\n",
    "        total_reward, gained_reward = rewards.action_based_reward(total_reward, crashed, action, y_pos, game_reward)\n",
    "\n",
    "        # store action infromation in memory\n",
    "        agent.remember(preprocessed_state, next_state_preprocessed, action, gained_reward, crashed)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        preprocessed_state = next_state_preprocessed\n",
    "        pbar.update(len(agent.memory.buffer) - pbar.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    print(f\"EPISODE: {episode}\")\n",
    "    # create env (human to render game and see actions)\n",
    "    if episode % RENDER_INTERVAL == 0 and episode != 0:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", render_mode=\"human\", difficulty=DIFFICULTY, mode=MODE) \n",
    "    else:\n",
    "        env = gym.make(EVIROMENT_VARIANT, obs_type=\"ram\", difficulty=DIFFICULTY, mode=MODE) \n",
    "        \n",
    "    state = env.reset()[0]\n",
    "    preprocessed_state = processing.preprocess_ram(state)\n",
    "\n",
    "    # reset episode variables\n",
    "    step_count = 0\n",
    "    distance_before = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # initilize episode stack\n",
    "    #stacked_array_state, STACKED_FRAMES = process.stack_frames(STACKED_FRAMES, preprocessed_state, is_new_episode, STATE_SIZE, STACKED_FRAMES_SIZE)\n",
    "    \n",
    "    while True:\n",
    "        # update step_count\n",
    "        step_count += 1\n",
    "\n",
    "        # Predict action\n",
    "        action = agent.predict_action(preprocessed_state)\n",
    "        next_state, game_reward, game_done, game_loss_of_live, game_info = env.step(action)\n",
    "\n",
    "        y_pos = env.ale.getRAM()[14] // 3\n",
    "        crashed = 1 if env.ale.getRAM()[16] != 255 else 0 # RAM(16) =:= Collision Lane\n",
    "\n",
    "        next_state_preprocessed = processing.preprocess_ram(next_state)\n",
    "\n",
    "        # update reward\n",
    "        total_reward, gained_reward = rewards.action_based_reward(total_reward, crashed, action, y_pos, game_reward)\n",
    "\n",
    "        # store action infromation in memory\n",
    "        agent.remember(preprocessed_state, next_state_preprocessed, action, gained_reward, crashed)\n",
    "\n",
    "        # set the next state to the current state\n",
    "        preprocessed_state = next_state_preprocessed\n",
    "\n",
    "        # Do Replay\n",
    "        agent.replay()\n",
    "\n",
    "        # END EPISODE IF CHICKEN COLLIDES\n",
    "        if crashed == 1 or game_reward == 1:\n",
    "            # clear output of cell for every new episode\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # update ntework\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            rewards_per_episode.append(total_reward)\n",
    "            distance_per_episode.append(y_pos)\n",
    "\n",
    "            if len(rewards_per_episode) >= AVERAGE_WINDOW:\n",
    "                moving_average = np.mean(rewards_per_episode[-AVERAGE_WINDOW:])\n",
    "            else:\n",
    "                moving_average = np.mean(rewards_per_episode)\n",
    "            average_rewards.append(moving_average)\n",
    "\n",
    "            step_text = f\"Episode: {episode}/{TOTAL_EPISODES}, Total Reward: {total_reward}, Moving AVG. Reward: {moving_average}, Distance: {y_pos}, Epsilon: {agent.EPSILON:.2}\"\n",
    "            \n",
    "            figures.reward_plot(rewards_per_episode, average_rewards)\n",
    "            figures.distance_plot(distance_per_episode)\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            print(step_text)\n",
    "            if episode % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            logger.log_episode(total_reward, moving_average, agent.EPSILON, y_pos)\n",
    "            networks.save_model(agent.target_model, episode, MODEL_SAVE_INTERVALL, dir=\"models/\")\n",
    "            break\n",
    "\n",
    "        is_new_episode = False\n",
    "        distance_before = y_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FreewayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
